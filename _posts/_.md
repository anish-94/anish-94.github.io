---
layout: post
toc: true
title:  "Pedestrian Tracking and Privacy Preservation (Senior Design Project)"
tags: [hot, summer]
image: "/images/pedestrian-counting-and-privacy-preservation.jpg"
permalink: /posts/capstone
---

The City of Portland is attempting to gather data on traffic patterns to increase pedestrian and driver safety and to create an open-source data-set on the topic. The Pedestrian Counting and Privacy Preservation project aids this goal by allowing the city to store video and photo information without storing identifying information about the people in the data. Serving as an archive of the steps and plans for Pavement Prometheus' project, the following document provides an overview of the project's past, current state, and goals for the future. The document will also cover the alpha release of the project and where the project will be at its beta release.

# Introduction

## The Problem

The City of Portland has partnered with companies such as AT\&T as part of the Smart City PDX initiative. The goal of the project is to use technology to better the lives of the city's citizens, specifically helping bridge the technological divide and help under-served communities. One aspect of the initiative is to use traffic and roadside cameras to develop open-source data-sets for the community to use for individual projects and for the city in the collection of data on traffic and pedestrian patterns. The data will be used to inform legislative and construction decisions as well as traffic decisions including traffic-light timing. The legislative action will be made in part to make public transit more efficient which will be targeted at underserved communities. Further, the open-data initiative will give these communities access to data for projects affecting them. The main problem that the city has run into is in storing the data. Currently, the data captured by the cameras has identifying information on people pictured in the media and, as a result, can not be stored for further analysis or use. The Pedestrian Counting and Privacy Preservation project primarily serves to fix this problem by stripping the incoming data of personally identifiable information so that it can be stored and further analyzed. As a secondary initiative, the Privacy Preservation project will provide access and storage for the data along with basic analysis of the data.

## Design

Our project has been split into four different components. Each component will each be worked on individually and combined in a pipeline to form an overarching solution to privacy preservation and data collection. The first component will be a real-time object detection system which, as its name suggests, will involve detecting objects in real-time from a given camera feed. For the purposes of this project we will be primarily interested in identifying both vehicles and pedestrians. The second component will be face detection, which will both serve to reinforce the accuracy of the object detection in identifying pedestrians, but will also be vital in obfuscation and in masking for privacy preservation. Our third component will involve an object tracking system, which will help extract interesting metrics for the data collection aspect of this project. Our object tracking system will also help ensure accuracy of aggregate data counts, as real-time object detection detects objects on a frame-by-frame basis and has no awareness of identifying a unique object between frames. Finally, we will have a data visualization and access component that will ultimately involve an online API for both storing and accessing the data collected to a database owned by the City of Portland.

## Goals

The goals of our project are to develop a program to detect pedestrians, deliver data in a JSON format to the City of Portland, and possibly develop a research paper based on our findings. When it comes to pedestrian detection, we hope to train a convolutional neural network to detect the bodies and faces of pedestrians in real-time. The results of this program will be analyzed and validated so pedestrians are detected with at least a 70 percent accuracy rate. Along with the face detection, the program will obscure the faces of those pedestrians so their identity is not compromised.

Our next goal is to deliver the pedestrian detection data in the form of a JSON file to the City of Portland. Our group will create a  API that can be used to aggregate and provice access to pedestrian data from our program in a JSON format. This will allow developers for the City of Portland to easily access and use the data from our program. The last goals are to create a method of elementary data analysis which the City of Portland can use as a metric for planning, and to optimize the tracking and detection algorithms. Since most current tools are made for general use, they are not optimized for the task we have set out to accomplish. The optimization step, if interesting and novel enough, may influence a written paper on the topic.                                         |
# Current State and Plans

## Object Detection

{% include image.html id="1" url="/public/images/ObjDetectFig.png" description="Video feed post Object Detection processing." %}

### Current State

The Object Detection portion of the capstone project is based primarily off You Only Look Once (YOLO), an open source detection algorithm which boasts real-time detection with an accuracy around 70% at an accompanying 20 frames per second. While this algorithm has remained largely intact leading up to the alpha release, additional functionality has been added for object obfuscation, simple data aggregation, Region of Interest (ROI) coordinate and image extraction, as well as basic communication with our web API service. Object obfuscation was the first simple modification applied to the YOLOv3 detection algorithm and entailed making use of YOLOv3's detected ROI in identifying a class of object. After detection an opaque rectangle encompassing the ROI is drawn, demonstrating a simple censorship method for personally identifiable information in a given video feed. The next modification to the YOLOv3 algorithm provided a basic count for vehicles and pedestrians in any given video frame, displaying their aggregate totals in the upper left-hand corner of the processed feed. To accommodate the inclusion of the Face Detection module, pedestrian ROI coordinate and image extraction was added under the premise that the face detection module might handle pedestrian obfuscation. This would ultimately allow for more precise censorship as opposed to the somewhat overly simple and arbitrary method of obfuscating the ROI entirely. The last functionality added for the alpha release was a simple request function for pushing data to the project's newly implemented web API. This data includes a detected object type, coordinates for said object`s ROI, and basic geospatial information regarding camera location. The current state of the video post processing can be seen in Figure 1.

### Plans

The next stages for the Object Detection module's development include training a new set of neural weights for our detection algorithm and the further incorporation of the Face Detection and Obfuscation module. While the default set of neural weights provided by the YOLOv3 engine are adequate for the purposes of the project's alpha release, it is necessary to train a set of weights solely for object classes directly related to the project`s goals. This is primarily because YOLOv3 provides detection for roughly 80 object classes while the project is interested in at most 10. Reducing the number of classes, then, should improve both detection speed and potentially accuracy. The incorporation of both the face and object detection modules will be one of the final stages of our project. At this stage object detection should have as near a complete coverage as possible, and with face detection and obfuscation any identifying details should be censored. However, we need to testing the integration step to determine how these modules should be incorporated e.g. Whether we should feed the face detection and obfuscation module directly from object detection or if the two should be run in tandem. The former might remove potential false positives, while the latter might allow for better obfuscation coverage for personally identifiable information at the cost of additional false positives.

### Goals

Real-time object detection is the first stage of analysis performed on a provided camera feed. This portion of the project is designed to implement an algorithm which can detect classes of objects in a timely manner which can then be fed to other subsystems for further analysis. To reach this objective, we will be implementing the You Only Look Once (YOLO) object detection algorithm on a PyTorch deep learning framework. While we are still looking for new datasets to train our model against, we originally wanted to use CARLA (an open source vehicle and pedestrian simulator which labels objects recorded automatically) to produce footage for training and, later, validating our model. We also have some interest in tweaking the YOLO object detection algorithm to improve accuracy and speed. Possible solutions to improving accuracy including reducing object classes recognized.

## Face Detection & Obfuscation

{% include image.html id="2" url="/public/images/scott.jpg" description="Video feed post Object Detection processing." %}

### Current State

The current facial detection program takes in an image and uses OpenCV features to detect the face of a pedestrian using a pre-trained convolutional neural network. The faces of pedestrians in an image are detected and the full image is output showing the confidence of the detection. In the output image, the faces are marked with a rectangle to point out their location so they are clearly seen. The faces are then obscured by a blurring function to ensure the identities of the pedestrians are removed. As planned, the facial detection used a feature-based method, so faces are detected easily from different perspectives and environments. This program has been tested with many different images of people and environments. The testing begins with a clear image of a person in a well-lit environment, although a broad array of images are tested to ensure the program can perform in a realistic environment. The faces tested vary in age, race, and angle in the images; and the environment consists of city streets, parks, and other outdoor areas. The program can detect faces with a high average of confidence when the image quality is well defined, but has more trouble detecting faces in the distance and from low quality pictures.

### Plans

The plans for the face detection program are to implement deep facial detection software for the facial detection rather than the current OpenCV features. OpenCV has worked great for the initial setup of the program, but our group wants to implement software that can detect faces better. Currently, OpenCV detects faces well if they are in a well-lit environment with clear image quality, but does not do as well with more realistic low-quality environments. The current foundation of the face detection program will be the same, but the detection feature will no longer be done by OpenCV to ensure more accurate results. Also, as the program is further integrated with the other project areas, the face detection program will use data from the object detection program and send output to the web API. Currently, the program works within its own functionality and can take output from the object detection program. The specifics of the program's input and output will need to be more precise given its role within the larger project. This means that the program will need to detect faces based on the coordinates passed to it within an image given by the object detection program and will need to output detection data to the web API for use. Other than those main two improvements, the program will continue to be tested with different images for confidence and accuracy.

### Goals

Facial detection serves our project goal of analyzing pedestrian movement while ensuring their identities and privacy are not compromised. While we detect the full bodies of pedestrians for motion and location data, the faces are detected for privacy matters. A human face is the most highly identifying part of a person, so our group detects it to be obscured. Through obscuring the face, all vital pedestrian data, including number and trajectory, is recorded except for the pedestrian's distinctive features. The traffic cameras that capture the footage used for the program cannot store data, so the program must be able to run in real time. Since our group would like the detection to be reliable and accurate, our goal is to detect pedestrians with at least a 70 percent confidence. The face detection should work seamlessly in coordination with the object detection and web API so our overall project is efficient and coherent.

## Data Analysis, Access, and Storage

A large portion of the data that the project will aggregate will be in a pure format e.g. the number of vehicles in a frame, their speed and trajectory, what lanes of highway contain the most traffic. This information, including the raw photographic and video data, is useful for constructing data sets, but is less useful for providing answers to decision problems. Analysis and visualization of the amassed information is essential in providing concrete advisement to the city.

The data will be analyzed using a mixture of comparative, statistical, and traffic  theory  based with the goal of providing information concerning traffic flow suggestions and traffic incidents. Analyzed data is currently accessible through a RESTful web API. The API currently serves data from a NOSQL-based mongo database in a JSON format including geographic and chronological markers for the data. The API also interfaces correctly with the object detection portion of the program pipeline. Furthermore, our team has created a simple web application for data visualization. The application currently updates a graph with datapoints aggregated from the aforementioned database.

The next steps for the Data part of the pipeline will be:

* Adding data analysis
* Making unit tests more encompassing
* Making the data visualization more aesthetically pleasing

An interesting feature of the database and API system is that the system is entirely contained within a Docker container. This ensures that the API and database can run internally on any system regardless of installed Python packages or database managers. The system can also be extracted to work with an external database as well.


# Problems

## Datasets

### General

Our group had multiple obstacles in our search to determine the best dataset to use for our project. First, we had to determine which format of data would serve all our needs, yet there seemed to be issues with each option. The City of Portland provided video feeds of different streets around Portland to take images from, but the image quality was poor and the live feed was too fragmented. Our group then looked into using a well-known, large dataset of faces to train our model, but just having that data would still lead to a lot of work. While our group would have a variety of face data to work with, we would need to individually label each pedestrian in an image. Finally, we investigated using a simulated environment to train our program with, but the software would need to be installed on the OSU server which wasn't completely possible. 

### CARLA

CARLA is a system for supporting the training and validation of  autonomous driving systems. CARLA is useful because it creates data that is a fair representation of traffic and pedestrian movement including pre-labeling of objects. CARLA also affords a lot of freedom by allowing large modifications with a powerful API layer. Our team hopes to use the system to help train our project. One problem that the team ran into while using the CARLA system was that the camera is meant to be situated in a car. This came about because the purpose of CARLA is for autonomous driving. Moving the camera to a stationary position broke many of the useful features in CARLA including automatic segmentation for cars and pedestrians, and ended up pausing the entire system.

For a decent amount of time, the automated pathing systems used for determining the trajectory and interaction between agents in the CARLA system was irregular. The pathing system caused cars to crash unexpectedly and caused pedestrians to walk into walls continuously and clash with each other in crowded situations. This could have had unforeseen effects on the finished system as CARLA data would have formed the base for our training.

## API

Unfortunately, through the team's interactions and correspondences with the City of Portland, many of the team's questions regarding the requirements for the API have gone unanswered. Specific question topics include the format of the API's output, information of specific interest, and access control. We have spoken with our primary client, Professor Li, about this issue and we have been advised to do whatever is necessary and correct in the implementation of the API.

A key problem that has come up is in the implementation of the access controls surrounding the API. From the perspective of a designer, our team knows that:

* The API and database will be open to the broad network
* IOT devices like roadside cameras will connect to the database through the API
* The API should generally be RESTful to help extensibility and to make development of further applications easier.

These points raise a few issues regarding the implementation of the API's access controls.

Beyond this, the project should be agnostic of what means are being used to access the API. This is an issue with ensuring the correctness and integrity of authentication attempts because the team won't be able to use public-key encryption for all data sent to-and-from the server. This is due to the ensuing slow down of communication between the IOT devices and the database.

{% highlight python %}
# import relevant libraries
import roslib
import rospy
import math

# The geometry_msgs Twist message
from geometry_msgs.msg import Twist

# The move_base result message
from move_base_msgs.msg import MoveBaseActionResult


def mb_callback(msg):
  # Check if robot has reached goal
  if msg.status.status == 2 or msg.status.status == 4 or msg.status.status == 5 or msg.status.status == 6:
    print "Robot failed to reach waypoint!"
  elif msg.status.status == 3:
    print "Robot successfully reached waypoint!"
  
  # Make a new Twist waypoint message
  waypoint = Twist()
  
  # Command waypoint 20 units to the right of the current robot position
  waypoint.linear.x = 0.0
  waypoint.linear.y = -20.0
  waypoint.linear.z = 0.0
  
  # Command the robot to turn 90 degrees clockwise
  waypoint.angular.x = 0.0
  waypoint.angular.y = 0.0
  waypoint.angular.z = -90.0
  
  pub.publish(waypoint)
{% endhighlight %}

# HH

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: http://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/

<!-- # References
{:.mt-2}

{% include reference.html
    id="1"
    details="G.  Hu,  Y.  Yang,  D.  Yi,  J.  Kittler,  W.  Christmas,  S.  Z.  Li,  and  T.  Hospedales,  “When  Face  Recognition  Meets  with  Deep  Learning:  an Evaluation of Convolutional Neural Networks for Face Recognition,”ArXiv e-prints, Apr. 2015"
    url="" 
%}
{% include reference.html
    id="2"
    details="G.  Hu,  Y.  Yang,  D.  Yi,  J.  Kittler,  W.  Christmas,  S.  Z.  Li,  and  T.  Hospedales,  “When  Face  Recognition  Meets  with  Deep  Learning:  an Evaluation of Convolutional Neural Networks for Face Recognition,”ArXiv e-prints, Apr. 2015"
    url="" 
%}
{% include reference.html
    id="3"
    details="G.  Hu,  Y.  Yang,  D.  Yi,  J.  Kittler,  W.  Christmas,  S.  Z.  Li,  and  T.  Hospedales,  “When  Face  Recognition  Meets  with  Deep  Learning:  an Evaluation of Convolutional Neural Networks for Face Recognition,”ArXiv e-prints, Apr. 2015"
    url="https://dl.acm.org/citation.cfm?id=2206309" 
%} -->